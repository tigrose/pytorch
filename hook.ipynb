{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banana/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "\n",
    "v = t.tensor(t.Tensor([0,0,0]), requires_grad=True)\n",
    "h = v.register_hook(lambda grad: grad*2) # return the double gradient\n",
    "v.backward(gradient=t.Tensor([1,1,1])) # you can remove the gradient to see what will happen\n",
    "\n",
    "print(v.grad.data)\n",
    "h.remove()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module hook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"楷体\">There are two kinds of hooks for module,one is forward hook, the other is backward hook.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input val: tensor([1.], requires_grad=True)\n",
      "output val: tensor(2., grad_fn=<SelectBackward>)\n",
      "model output is: tensor([2.], grad_fn=<AddBackward0>)\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banana/.local/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# 1. forward_hook(hook)\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def for_hook(module, input, output): ## -> should return None\n",
    "    global mark_modu\n",
    "    mark_modu = module\n",
    "    for val in input:\n",
    "        print(\"input val:\",val)\n",
    "    for out_val in output:\n",
    "        print(\"output val:\", out_val)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x+1\n",
    "\n",
    "model = Model()\n",
    "x = torch.tensor(torch.Tensor([1]), requires_grad=True)\n",
    "handle = model.register_forward_hook(for_hook) # return a handle\n",
    "print(\"model output is:\",model(x))\n",
    "handle.remove() # remove the hook\n",
    "print(mark_modu(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result =  tensor([20.5000], grad_fn=<DivBackward0>)\n",
      "doing_my_hook\n",
      "original grad (tensor([0.2500]), None)\n",
      "original outgrad (tensor([1.]),)\n",
      "tensor hook\n",
      "grad: tensor([2., 2., 2., 2.])\n",
      "input.grad tensor([2., 2., 2., 2.])\n",
      "Parameter containing:\n",
      "tensor([[8., 8., 8., 8.]], requires_grad=True):grad->tensor([[0.2500, 0.5000, 0.7500, 1.0000]])\n",
      "Parameter containing:\n",
      "tensor([2.], requires_grad=True):grad->tensor([0.2500])\n"
     ]
    }
   ],
   "source": [
    "# 2. backward hook\n",
    "# register_backward_hook(hook) -> return a Tensor or None\n",
    "# we can change the data of grad_inputs and return it\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def tensor_hook(grad):\n",
    "    print('tensor hook')\n",
    "    print('grad:', grad)\n",
    "    return grad\n",
    "\n",
    "class MyMean(nn.Module):            # 自定义除法module\n",
    "    def forward(self, input):\n",
    "        out = input/4\n",
    "        return out\n",
    "    \n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.f1 = nn.Linear(4, 1, bias=True)\n",
    "        self.f2 = MyMean()\n",
    "        self.weight_init()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        output = self.f1(input)\n",
    "        output = self.f2(output)\n",
    "        return output\n",
    "    \n",
    "    def weight_init(self):\n",
    "        self.f1.weight.data.fill_(8.0)\n",
    "        self.f1.bias.data.fill_(2.0)\n",
    "    \n",
    "    def my_hook(self, module, grad_input, grad_output):\n",
    "        print(\"doing_my_hook\")\n",
    "        print('original grad', grad_input)\n",
    "        print('original outgrad', grad_output)\n",
    "        \n",
    "        return grad_input\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "        \n",
    "    input = torch.tensor([1, 2, 3, 4], dtype=torch.float32, requires_grad=True).to(device)\n",
    "    net = MyNet()\n",
    "    net.register_backward_hook(net.my_hook)\n",
    "    \n",
    "    input.register_hook(tensor_hook)\n",
    "    result = net(input)\n",
    "    print('result = ',result)\n",
    "    result.backward()\n",
    "    print('input.grad', input.grad)\n",
    "    for param in net.parameters():\n",
    "        print('{}:grad->{}'.format(param, param.grad))\n",
    "# here y = w1*x1 + ...  + 24*x4 +b\n",
    "# z = y/4,so the grad_input is 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result = tensor([22.], grad_fn=<AddBackward0>)\n",
      "doing my_hook\n",
      "original grad: (tensor([1.]), tensor([1.]))\n",
      "original outgrad: (tensor([1.]),)\n",
      "tensor hook\n",
      "grad: tensor([2., 2., 2., 2.])\n",
      "input.grad: tensor([2., 2., 2., 2.])\n",
      "Parameter containing:\n",
      "tensor([[8., 8., 8., 8.]], requires_grad=True):grad->tensor([[0.2500, 0.5000, 0.7500, 1.0000]])\n",
      "Parameter containing:\n",
      "tensor([2.], requires_grad=True):grad->tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "class MyMul(nn.Module):\n",
    "    def forward(self, input):\n",
    "        out = input * 2\n",
    "        return out\n",
    "class MyMean(nn.Module):            # 自定义除法module\n",
    "    def forward(self, input):\n",
    "        out = input/4\n",
    "        return out\n",
    "def tensor_hook(grad):\n",
    "    print('tensor hook')\n",
    "    print('grad:', grad)\n",
    "    return grad\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.f1 = nn.Linear(4, 1, bias=True)    \n",
    "        self.f2 = MyMean()\n",
    "        self.weight_init()\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        output = self.f2(input)       # 先进行运算1，后进行运算2\n",
    "        output = self.f1(output)      \n",
    "        return output\n",
    "    def weight_init(self):\n",
    "        self.f1.weight.data.fill_(8.0)    # 这里设置Linear的权重为8\n",
    "        self.f1.bias.data.fill_(2.0)      # 这里设置Linear的bias为2\n",
    "    def my_hook(self, module, grad_input, grad_output):\n",
    "        print('doing my_hook')\n",
    "        print('original grad:', grad_input)\n",
    "        print('original outgrad:', grad_output)\n",
    "        # grad_input = grad_input[0]*self.input   # 这里把hook函数内对grad_input的操作进行了注释，\n",
    "        # grad_input = tuple([grad_input])        # 返回的grad_input必须是tuple，所以我们进行了tuple包装。\n",
    "        # print('now grad:', grad_input)        \n",
    "        return grad_input\n",
    "if __name__ == '__main__':\n",
    "    input = torch.tensor([1, 2, 3, 4], dtype=torch.float32, requires_grad=True).to(device)\n",
    "    net = MyNet()\n",
    "    net.to(device)\n",
    "    net.register_backward_hook(net.my_hook)   # 这两个hook函数一定要result = net(input)执行前执行，因为hook函数实在forward的时候进行绑定的\n",
    "    input.register_hook(tensor_hook)\n",
    "    result = net(input)\n",
    "    print('result =', result)\n",
    "    result.backward()\n",
    "    print('input.grad:', input.grad)\n",
    "    for param in net.parameters():\n",
    "        print('{}:grad->{}'.format(param, param.grad))\n",
    "\n",
    "# here z=y1*w1 + ... + y4*w4 +b, torch treats the formul as z = m + b, where m = y1*w1 + ... + y4*w4\n",
    "# so the grad_input is 1,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result = tensor([5.5000], grad_fn=<DivBackward0>)\n",
      "doing my_hook\n",
      "original grad: (tensor([0.2500]), None)\n",
      "original outgrad: (tensor([1.]),)\n",
      "tensor hook\n",
      "grad: tensor([0.5000, 0.5000, 0.5000, 0.5000])\n",
      "input.grad: tensor([0.5000, 0.5000, 0.5000, 0.5000])\n",
      "Parameter containing:\n",
      "tensor([[8., 8., 8., 8.]], requires_grad=True):grad->tensor([[0.0625, 0.1250, 0.1875, 0.2500]])\n",
      "Parameter containing:\n",
      "tensor([2.], requires_grad=True):grad->tensor([0.2500])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "class MyMul(nn.Module):\n",
    "    def forward(self, input):\n",
    "        out = input * 2\n",
    "        return out\n",
    "class MyMean(nn.Module):            # 自定义除法module\n",
    "    def forward(self, input):\n",
    "        out = input/4\n",
    "        return out\n",
    "def tensor_hook(grad):\n",
    "    print('tensor hook')\n",
    "    print('grad:', grad)\n",
    "    return grad\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.f1 = nn.Linear(4, 1, bias=True)    \n",
    "        self.f2 = MyMean()\n",
    "        self.weight_init()\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        output = self.f2(input)       # 先进行运算1，后进行运算2\n",
    "        output = self.f1(output) / 4      \n",
    "        return output\n",
    "    def weight_init(self):\n",
    "        self.f1.weight.data.fill_(8.0)    # 这里设置Linear的权重为8\n",
    "        self.f1.bias.data.fill_(2.0)      # 这里设置Linear的bias为2\n",
    "    def my_hook(self, module, grad_input, grad_output):\n",
    "        print('doing my_hook')\n",
    "        print('original grad:', grad_input)\n",
    "        print('original outgrad:', grad_output)\n",
    "        # grad_input = grad_input[0]*self.input   # 这里把hook函数内对grad_input的操作进行了注释，\n",
    "        # grad_input = tuple([grad_input])        # 返回的grad_input必须是tuple，所以我们进行了tuple包装。\n",
    "        # print('now grad:', grad_input)        \n",
    "        return grad_input\n",
    "if __name__ == '__main__':\n",
    "    input = torch.tensor([1, 2, 3, 4], dtype=torch.float32, requires_grad=True).to(device)\n",
    "    net = MyNet()\n",
    "    net.to(device)\n",
    "    net.register_backward_hook(net.my_hook)   # 这两个hook函数一定要result = net(input)执行前执行，因为hook函数实在forward的时候进行绑定的\n",
    "    input.register_hook(tensor_hook)\n",
    "    result = net(input)\n",
    "    print('result =', result)\n",
    "    result.backward()\n",
    "    print('input.grad:', input.grad)\n",
    "    for param in net.parameters():\n",
    "        print('{}:grad->{}'.format(param, param.grad))\n",
    "\n",
    "# here z = (y1*w1 + ... + y4*w4 +b) / 4, torch treats formul as z = y/4\n",
    "# so grad_input is 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summarize\n",
    "<font face=\"楷体\">backward hook will automaticly calculate the last layer </font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
