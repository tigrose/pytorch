{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未人工初始化之前w的权重： Parameter containing:\n",
      "tensor([[[[ 0.0938,  0.1979,  0.2250],\n",
      "          [ 0.0160,  0.0163,  0.0819],\n",
      "          [ 0.1533,  0.0057, -0.1118]],\n",
      "\n",
      "         [[ 0.1532, -0.1358,  0.1617],\n",
      "          [ 0.2159, -0.1377,  0.2163],\n",
      "          [ 0.2129,  0.1197,  0.0804]]],\n",
      "\n",
      "\n",
      "        [[[-0.0150, -0.0840,  0.0984],\n",
      "          [ 0.1779, -0.0160,  0.0363],\n",
      "          [ 0.1586, -0.1000,  0.2185]],\n",
      "\n",
      "         [[-0.2212, -0.1584,  0.1141],\n",
      "          [-0.1059,  0.0340,  0.1892],\n",
      "          [-0.0202,  0.0322,  0.0530]]]], requires_grad=True)\n",
      "1. 使用另一个Conv层的权重初始化\n",
      "q的权重 Parameter containing:\n",
      "tensor([[[[ 0.1820,  0.1627,  0.1445],\n",
      "          [ 0.1745, -0.1787,  0.0869],\n",
      "          [ 0.1917, -0.0720, -0.0945]],\n",
      "\n",
      "         [[-0.1909, -0.0782, -0.1951],\n",
      "          [ 0.2165,  0.1200, -0.2032],\n",
      "          [-0.1952,  0.1195,  0.0949]]],\n",
      "\n",
      "\n",
      "        [[[-0.2200, -0.0828, -0.1513],\n",
      "          [-0.0629, -0.0266, -0.0422],\n",
      "          [ 0.0726,  0.0742,  0.2218]],\n",
      "\n",
      "         [[ 0.1315, -0.1648, -0.1675],\n",
      "          [ 0.1944, -0.0856,  0.1877],\n",
      "          [-0.1307, -0.1916,  0.1700]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.1820,  0.1627,  0.1445],\n",
      "          [ 0.1745, -0.1787,  0.0869],\n",
      "          [ 0.1917, -0.0720, -0.0945]],\n",
      "\n",
      "         [[-0.1909, -0.0782, -0.1951],\n",
      "          [ 0.2165,  0.1200, -0.2032],\n",
      "          [-0.1952,  0.1195,  0.0949]]],\n",
      "\n",
      "\n",
      "        [[[-0.2200, -0.0828, -0.1513],\n",
      "          [-0.0629, -0.0266, -0.0422],\n",
      "          [ 0.0726,  0.0742,  0.2218]],\n",
      "\n",
      "         [[ 0.1315, -0.1648, -0.1675],\n",
      "          [ 0.1944, -0.0856,  0.1877],\n",
      "          [-0.1307, -0.1916,  0.1700]]]], requires_grad=True)\n",
      "2. 使用来自Tensor的权值\n",
      "Parameter containing:\n",
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "w = torch.nn.Conv2d(2,2,3,padding=1)\n",
    "print(\"未人工初始化之前w的权重：\",w.weight)\n",
    "\n",
    "print(\"1. 使用另一个Conv层的权重初始化\")\n",
    "q = torch.nn.Conv2d(2,2,3,padding=1)\n",
    "print(\"q的权重\",q.weight)\n",
    "w.weight=q.weight\n",
    "print(w.weight)\n",
    "\n",
    "print(\"2. 使用来自Tensor的权值\")\n",
    "ones = torch.Tensor(np.ones([2,2,3,3]))\n",
    "w.weight = torch.nn.Parameter(ones)\n",
    "print(w.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Variable和Parameter的区别\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter是Variable的一个子类，经常被用于Module的参数，比如weight权值和bias偏重。\n",
    "\n",
    "Parameter和Module一起使用的时候会有一些特殊的属性，当Parameter赋值给module的属性的时候，\n",
    "它会自动被加到Module的参数列表中，即会出现在Parameter()的迭代器中。\n",
    "而将Variable赋给Module的时候没有这样的属性。这样是为了保存模型的时候只保存权值和偏置参数。\n",
    "`另外，Parameter的requires_grad默认为True，而Variable默认为False。`\n",
    "\n",
    "Parameter.data - > 得到 tensor 数据\n",
    "Parameter.requires_grad - > True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 几种常用的初始化方法\n",
    " --- \n",
    " ```python\n",
    "     torch.nn.init.xavier_normal_()\n",
    "     torch.nn.init.xavier_uniform_()\n",
    "     torch.nn.init.normal_()\n",
    "     torch.nn.init.constant_()\n",
    "     layer.weight.data.normal_(mean,std)\n",
    "     layer.bias.data.fill_(1)\n",
    " ```\n",
    "这些方法既可以对tensor进行初始化，也可以对Variable以及Parameter进行初始化。\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.4691e-37, 0.0000e+00, 1.4013e-45]])\n",
      "tensor([[2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "# 对tensor进行初始化\n",
    "w = torch.FloatTensor(1,3)\n",
    "print(w)\n",
    "torch.nn.init.constant_(w,2)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 0.2038, -0.1263,  0.1419],\n",
      "          [ 0.1426,  0.1871, -0.1218],\n",
      "          [-0.0912, -0.0282, -0.1751]],\n",
      "\n",
      "         [[-0.1216, -0.1523, -0.1610],\n",
      "          [ 0.0008, -0.0028,  0.1226],\n",
      "          [-0.2235, -0.1028, -0.0474]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0685,  0.2059, -0.0564],\n",
      "          [ 0.1313, -0.0621, -0.0822],\n",
      "          [-0.2014, -0.0239, -0.0645]],\n",
      "\n",
      "         [[-0.0972, -0.0697,  0.0499],\n",
      "          [ 0.1692,  0.1729,  0.2323],\n",
      "          [-0.1839,  0.0887,  0.0602]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.3845, -0.1396, -0.4007],\n",
      "          [-0.1431, -0.1594,  0.1285],\n",
      "          [-0.3629, -0.1895,  0.2286]],\n",
      "\n",
      "         [[-0.1719,  0.3718, -0.2272],\n",
      "          [ 0.1165, -0.3947, -0.3502],\n",
      "          [-0.1379, -0.1323, -0.3598]]],\n",
      "\n",
      "\n",
      "        [[[-0.1687, -0.1742,  0.3303],\n",
      "          [-0.3114,  0.3304, -0.0697],\n",
      "          [-0.1152,  0.2530, -0.0241]],\n",
      "\n",
      "         [[ 0.0510, -0.1412,  0.0053],\n",
      "          [-0.3865, -0.4049, -0.3436],\n",
      "          [ 0.0507, -0.2821,  0.0858]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 对某一层的参数进行初始化\n",
    "import torch\n",
    "w = torch.nn.Conv2d(2,2,3,padding=1)\n",
    "print(w.weight)\n",
    "torch.nn.init.xavier_uniform_(w.weight)\n",
    "print(w.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_dict = ...\n",
    "model_dict = model.state_dict()\n",
    "\n",
    "pretrained_dict = {k:v for k, v in pretrained_dict.items() if k in model_dict} # 剔除掉原来模型中不存在的键值对\n",
    "\n",
    "# 更新model_dict\n",
    "model_dict.update(pretrained_dict)\n",
    "\n",
    "# 将model_dict载入model模型中\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module.modules()与Module.children()\n",
    "---\n",
    "modules()会以迭代器的形式返回网络中所有模块\n",
    "\n",
    "而children只会返回所有直接子模块的一个iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sequential(\n",
      "  (0): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (1): ReLU()\n",
      "), Linear(in_features=10, out_features=20, bias=True), ReLU()]\n",
      "[Linear(in_features=10, out_features=20, bias=True), ReLU()]\n"
     ]
    }
   ],
   "source": [
    "print(list(torch.nn.Sequential(nn.Linear(10,20), nn.ReLU()).modules()))\n",
    "\n",
    "print(list(torch.nn.Sequential(nn.Linear(10,20), nn.ReLU()).children()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 固定部分参数进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "param_optim\n",
    "for k in model.children():\n",
    "    count += 1 \n",
    "    if count > 5:\n",
    "        for param in k.parameters()\n",
    "            param_optim.append(param)\n",
    "    else:\n",
    "        for param in k.parameters():\n",
    "            param.requires_grad = False\n",
    "optimizer = torch.optim.SGD(param_optim, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法二"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,6,5)\n",
    "        self.conv2 = nn.Conv2d(1,3,4)\n",
    "        \n",
    "        # 以上的网络层不进行训练，固定参数\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad=False\n",
    "        \n",
    "        #　继续添加网络层\n",
    "        self.fc1 = nn.Linear(20,30)\n",
    "        self.fc2 = nn.Linear(20,30)\n",
    "        self.fc3 = nn.Linear(20,30)\n",
    "\n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
